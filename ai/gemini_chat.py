import google.generativeai as genai
import asyncio


class GeminiChat:
    """Class for interacting with the Gemini language model via the Google Generative AI API.

    This class provides functionality to start a new chat session and retrieve responses
    asynchronously from the Gemini model.

    Attributes:
        api_key (str): The API key used to authenticate requests to the Google Generative AI API.
        model_name (str): The name of the model to use for generating responses (default is "gemini-1.5-flash").
        model (genai.GenerativeModel): The model instance used to generate responses.
        chat (genai.ChatSession): The chat session instance used to maintain conversation context.
    """

    def __init__(self, api_key: str, model_name: str = "gemini-1.5-flash") -> None:
        """Initializes the GeminiChat class with the provided API key and model name.

        Args:
            api_key (str): The API key for authenticating requests to the Google Generative AI API.
            model_name (str, optional): The model name to use for generating responses. Defaults to "gemini-1.5-flash".
        """
        self.api_key: str = api_key
        self.model_name: str = model_name

        self.model: genai.GenerativeModel = None
        self.chat: genai.ChatSession = None
    
    def start_new_chat(self, role_prompt: str, model_name: str = None) -> None:
        """Starts a new chat session with the Gemini model, using the provided role prompt.

        Args:
            role_prompt (str): The initial prompt that sets the context or role for the chat session.
            model_name (str, optional): The model name to use for this session. If not provided,
                                        the default model name will be used.

        Raises:
            Exception: If an error occurs during the chat session initialization.
        """
        try:
            genai.configure(api_key=self.api_key)
            self.model_name = model_name or self.model_name

            self.model = genai.GenerativeModel(self.model_name)
            self.chat = self.model.start_chat(history=[{"role": "user", "parts": role_prompt}])

        except Exception as e:
            print(f"An error occurred while starting the chat: {e}")
    
    async def get_chat_response(self, user_prompt: str) -> str:
        """Sends a user prompt to the current chat session and returns the model's response asynchronously.

        Args:
            user_prompt (str): The user's prompt or message to send to the chat session.

        Returns:
            str: The response text generated by the Gemini model.

        Raises:
            Exception: If an error occurs during the message sending or response retrieval.
        """
        try:
            response = await self.chat.send_message_async(user_prompt)
            return response.text

        except Exception as e:
            print(f"An error occurred: {e}")
            return "An unexpected error occurred."